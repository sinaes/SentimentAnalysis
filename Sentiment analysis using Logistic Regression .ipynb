{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re, nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer        \n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"Sentimental training dataset.txt\", header=None, delimiter=\"\\t\", quoting=3)\n",
    "train_df.columns = [\"Sentiment\",\"Text\"]\n",
    "test_df = pd.read_csv(\"Sentimental test dataset.txt\", header=None, delimiter=\"\\t\", quoting=3)\n",
    "test_df.columns = [\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                               Text\n",
       "0          1            The Da Vinci Code book is just awesome.\n",
       "1          1  this was the first clive cussler i've ever rea...\n",
       "2          1                   i liked the Da Vinci Code a lot.\n",
       "3          1                   i liked the Da Vinci Code a lot.\n",
       "4          1  I liked the Da Vinci Code but it ultimatly did..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\" I don't care what anyone says, I like Hillar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>have an awesome time at purdue!..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yep, I'm still in London, which is pretty awes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Have to say, I hate Paris Hilton's behavior bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i will love the lakers.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  \" I don't care what anyone says, I like Hillar...\n",
       "1                  have an awesome time at purdue!..\n",
       "2  Yep, I'm still in London, which is pretty awes...\n",
       "3  Have to say, I hate Paris Hilton's behavior bu...\n",
       "4                            i will love the lakers."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how many labels do we have for each sentiment class and what are their percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3995\n",
       "0    3091\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    56.378775\n",
       "0    43.621225\n",
       "Name: Sentiment, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_df.Sentiment.value_counts()/len(train_df))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's calculate the average number of words per sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.886819079875812"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(s.split(\" \")) for s in train_df.Text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a corpus\n",
    "The class sklearn.feature_extraction.text.CountVectorizer in the wonderful scikit learn Python library converts a collection of text documents to a matrix of token counts. This is just what we need to implement later on our bag-of-words linear classifier.\n",
    "First we need to init the vectorizer. We need to remove punctuations, lowercase, remove stop words, and stem words. All these steps can be directly performed by CountVectorizer if we pass the right parameter values. We can do as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#######\n",
    "# based on http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    # remove non letters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # stem\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "######## \n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = 'english',\n",
    "    max_features = 85\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_data_features = vectorizer.fit_transform(train_df.Text.tolist() + test_df.Text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40138, 85)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_data_features_nd = corpus_data_features.toarray()\n",
    "corpus_data_features_nd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the words in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaa', 'amaz', 'angelina', 'awesom', 'beauti', 'becaus', 'boston', 'brokeback', 'citi', 'code', 'cool', 'cruis', 'd', 'da', 'drive', 'francisco', 'friend', 'fuck', 'geico', 'good', 'got', 'great', 'ha', 'harri', 'harvard', 'hate', 'hi', 'hilton', 'honda', 'imposs', 'joli', 'just', 'know', 'laker', 'left', 'like', 'littl', 'london', 'look', 'lot', 'love', 'm', 'macbook', 'make', 'miss', 'mission', 'mit', 'mountain', 'movi', 'need', 'new', 'oh', 'onli', 'pari', 'peopl', 'person', 'potter', 'purdu', 'realli', 'right', 'rock', 's', 'said', 'san', 'say', 'seattl', 'shanghai', 'stori', 'stupid', 'suck', 't', 'thi', 'thing', 'think', 'time', 'tom', 'toyota', 'ucla', 've', 'vinci', 'wa', 'want', 'way', 'whi', 'work']\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the counts of each word in the vocabulary as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1179 aaa\n",
      "485 amaz\n",
      "1765 angelina\n",
      "3170 awesom\n",
      "2146 beauti\n",
      "1694 becaus\n",
      "2190 boston\n",
      "2000 brokeback\n",
      "423 citi\n",
      "2003 code\n",
      "481 cool\n",
      "2031 cruis\n",
      "439 d\n",
      "2087 da\n",
      "433 drive\n",
      "1926 francisco\n",
      "477 friend\n",
      "452 fuck\n",
      "1085 geico\n",
      "773 good\n",
      "571 got\n",
      "1178 great\n",
      "776 ha\n",
      "2094 harri\n",
      "2103 harvard\n",
      "4492 hate\n",
      "794 hi\n",
      "2086 hilton\n",
      "2192 honda\n",
      "1098 imposs\n",
      "1764 joli\n",
      "1054 just\n",
      "896 know\n",
      "2019 laker\n",
      "425 left\n",
      "4080 like\n",
      "507 littl\n",
      "2233 london\n",
      "811 look\n",
      "421 lot\n",
      "10334 love\n",
      "1568 m\n",
      "1059 macbook\n",
      "631 make\n",
      "1098 miss\n",
      "1101 mission\n",
      "1340 mit\n",
      "2081 mountain\n",
      "1207 movi\n",
      "1220 need\n",
      "459 new\n",
      "551 oh\n",
      "674 onli\n",
      "2094 pari\n",
      "1018 peopl\n",
      "454 person\n",
      "2093 potter\n",
      "1167 purdu\n",
      "2126 realli\n",
      "661 right\n",
      "475 rock\n",
      "3914 s\n",
      "495 said\n",
      "2038 san\n",
      "627 say\n",
      "2019 seattl\n",
      "1189 shanghai\n",
      "467 stori\n",
      "2886 stupid\n",
      "4614 suck\n",
      "1455 t\n",
      "1705 thi\n",
      "662 thing\n",
      "1524 think\n",
      "781 time\n",
      "2117 tom\n",
      "2028 toyota\n",
      "2008 ucla\n",
      "774 ve\n",
      "2001 vinci\n",
      "3703 wa\n",
      "1656 want\n",
      "932 way\n",
      "547 whi\n",
      "512 work\n"
     ]
    }
   ],
   "source": [
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(corpus_data_features_nd, axis=0)\n",
    "    \n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the data set\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print(count,tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform logistic regression to our training dataset, first we need to split our training data to get an evaluation set. The problem with not having labels in our original test set still persists, and we need to create a separate evaluation set from our original training data if we want to evaluate our classifier. We will use train_test_split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sina\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "    \n",
    "# remember that corpus_data_features_nd contains all of our \n",
    "# original train and test data, so we need to exclude\n",
    "# the unlabeled test entries\n",
    "X_train, X_test, y_train, y_test  = train_test_split(\n",
    "        corpus_data_features_nd[0:len(train_df)], \n",
    "        train_df.Sentiment,\n",
    "        train_size=0.85, \n",
    "        random_state=1234)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "log_model = LogisticRegression()\n",
    "log_model = log_model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And once the model has been created, we can predict the Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.98       467\n",
      "          1       0.99      0.98      0.99       596\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1063\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can re-train our model with all the training data and use it for\n",
    "sentiment classification with the original (unlabeled) test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 have an awesome time at purdue!..\n",
      "0 Lakers suck under pressure and they don't win unless Kobe passes the ball which I doubt will happen in game 7.\n",
      "1 Purdue's is quite ugly, however, I'm a boilermaker and always will be despite the outlook;\n",
      "1 Three days at Purdue with three awesome people.\n",
      "1 My Purdue Cal friends are awesome!..\n",
      "0 i try to enjoy my time here in san francisco.......\n",
      "0 I've been working on an article, and Antid Oto has been, er, so upset about the shitty Harvard plagiarizer that he hasn't been able to even look at keyboards.\n",
      "1 Proud parents Tom Cruise, 44, and Katie Holmes, 27, were on the cover with their beautiful baby girl.\n",
      "0 Stupid UCLA.\n",
      "1 I also love the new rabbits, I still want an x-terra-but Luke's will do, and I kinda like Honda Elements, but they got a bad safety rating: (..\n",
      "0 UCLA is stupid, I realized.\n",
      "1 i love Kappo Honda, which is across the street..\n",
      "0 gawssh i hate london i hope he blows up and his guts fly everywhere and then birds eat his guts.\n",
      "0 UCLA is sucking up the game big time...\n",
      "1 State Farm is taking good care of them, but they may be living in an apartment for a while.\n"
     ]
    }
   ],
   "source": [
    "# train classifier\n",
    "log_model = LogisticRegression()\n",
    "log_model = log_model.fit(X=corpus_data_features_nd[0:len(train_df)], y=train_df.Sentiment)\n",
    "    \n",
    "# get predictions\n",
    "test_pred = log_model.predict(corpus_data_features_nd[len(train_df):])\n",
    "    \n",
    "# sample some of them\n",
    "import random\n",
    "spl = random.sample(range(len(test_pred)), 15)\n",
    "    \n",
    "# print text and labels\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "    print(sentiment, text)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
